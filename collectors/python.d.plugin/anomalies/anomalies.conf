# netdata python.d.plugin configuration for example
#
# This file is in YaML format. Generally the format is:
#
# name: value
#
# There are 2 sections:
#  - global variables
#  - one or more JOBS
#
# JOBS allow you to collect values from multiple sources.
# Each source will have its own set of charts.
#
# JOB parameters have to be indented (using spaces only, example below).

# ----------------------------------------------------------------------
# Global Variables
# These variables set the defaults for all JOBs, however each JOB
# may define its own, overriding the defaults.

# update_every sets the default data collection frequency.
# If unset, the python.d.plugin default is used.
update_every: 2

# priority controls the order of charts at the netdata dashboard.
# Lower numbers move the charts towards the top of the page.
# If unset, the default for python.d.plugin is used.
# priority: 60000

# penalty indicates whether to apply penalty to update_every in case of failures.
# Penalty will increase every 5 failed updates in a row. Maximum penalty is 10 minutes.
# penalty: yes

# autodetection_retry sets the job re-check interval in seconds.
# The job is not deleted if check fails.
# Attempts to start the job are made once every autodetection_retry.
# This feature is disabled by default.
# autodetection_retry: 0

# ----------------------------------------------------------------------
# JOBS (data collection sources)
#
# The default JOBS share the same *name*. JOBS with the same name
# are mutually exclusive. Only one of them will be allowed running at
# any time. This allows autodetection to try several alternatives and
# pick the one that works.
#
# Any number of jobs is supported.
#
# All python.d.plugin JOBS (for all its modules) support a set of
# predefined parameters. These are:
#
# job_name:
#     name: myname            # the JOB's name as it will appear at the
#                             # dashboard (by default is the job_name)
#                             # JOBs sharing a name are mutually exclusive
#     update_every: 1         # the JOB's data collection frequency
#     priority: 60000         # the JOB's order on the dashboard
#     penalty: yes            # the JOB's penalty
#     autodetection_retry: 0  # the JOB's re-check interval in seconds
#
# Additionally to the above, example also supports the following:
#
# - none
#
# ----------------------------------------------------------------------
# AUTO-DETECTION JOBS
# only one of them will run (they have the same name)

# use http or https to pull data
protocol: 'http'
# what host to pull data from.
host: '127.0.0.1:19999'
# what charts to pull data for - A regex like 'system\..*|' or 'system\..*|apps.cpu|apps.mem' etc.
charts_in_scope: 'system\..*'
# what model to use - can be one of 'pca', 'hbos', 'iforest', 'cblof', 'loda', 'copod' or 'feature_bagging'. More details here: https://pyod.readthedocs.io/en/latest/pyod.models.html.
model: 'pca'
# max number of observations to train on, to help cap compute cost of training model if you set a very large train_n_secs.
train_max_n: 100000
# how often to re-train the model (assuming update_every=2 then train_every_n=900 represents (re)training every 30 minutes).
train_every_n: 900
# the length of the window of data to train on (14400 = last 4 hours).
train_n_secs: 14400
# if you would like to ignore recent data in training then you can offset it by offset_n_secs.
offset_n_secs: 0
# how many lagged values of each dimension to include in the 'feature vector' each model is trained on.
lags_n: 5
# how much smoothing to apply to each dimension in the 'feature vector' each model is trained on.
smooth_n: 3
# how many differences to take in preprocessing your data. diffs_n=0 would mean training models on the raw values of each dimension, whereas diffs_n=1 means everything is done in terms of differences.
diffs_n: 1
# What is the typical proportion of anomalies in your data on average? This paramater can control the sensitivity of your models to anomalies. Some discussion here: https://github.com/yzhao062/pyod/issues/144
contamination: 0.001
# define any custom models you would like to create anomaly probabilties for, some examples below to show how.
# for example below example creates two custom models, one to run anomaly detection on the netdata user and one on the apps metrics for python.d.plugin.
#custom_models:
# - name: 'user_netdata'
#   dimensions: 'users.cpu|netdata,users.mem|netdata,users.threads|netdata,users.processes|netdata,users.sockets|netdata'
# - name: 'apps_python_d_plugin'
#   dimensions: 'apps.cpu|python.d.plugin,apps.mem|python.d.plugin,apps.threads|python.d.plugin,apps.processes|python.d.plugin,apps.sockets|python.d.plugin'
# set to true to normalize, using min-max standardization, features used for the custom models. Useful if your custom models contain dimensions on very different scales. Usually best to leave as false.
#custom_models_normalize: false
